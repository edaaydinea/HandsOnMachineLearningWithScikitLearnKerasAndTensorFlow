{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "For datasets with **millions of features**, **Stochastic Gradient Descent (SGD)** or **Mini-batch Gradient Descent** are commonly used because they are efficient for large datasets, both in terms of memory usage and computational speed. In contrast, methods like **Normal Equation** or **Batch Gradient Descent** can be computationally expensive for such large-scale problems due to their complexity (inversion of large matrices, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?\n",
    "\n",
    "Algorithms that use **Gradient Descent** (like Linear Regression, Logistic Regression, and Neural Networks) can suffer from features with different scales because the gradients might become unbalanced, making the algorithm **converge more slowly** or struggle to converge at all. \n",
    "\n",
    "Other algorithms like **SVMs** or **KNN** also rely on distance calculations, which can be impacted by feature scales.\n",
    "\n",
    "**Solution**: **Feature scaling** is the answer—standardize or normalize the features (e.g., using StandardScaler in Scikit-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "\n",
    "No, **Gradient Descent** cannot get stuck in a local minimum for **Logistic Regression**, as its cost function (the log-loss function) is convex. Convex functions have a single global minimum, so Gradient Descent will always converge to this global minimum (assuming the learning rate is properly set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Do all Gradient Descent algorithms lead to the same model, provided you let them run long enough?\n",
    "\n",
    "Yes, **all Gradient Descent algorithms** (Batch, Stochastic, Mini-batch) will theoretically converge to the same optimal model if you let them run long enough with a sufficiently small learning rate. However, they may take different amounts of time to converge due to their step-size behavior:\n",
    "- **Batch GD** may take fewer epochs but is computationally expensive.\n",
    "- **Stochastic and Mini-batch GD** can fluctuate more and take longer but are faster per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "If the **validation error** goes up consistently, it's likely that the model is **overfitting** the training data.\n",
    "\n",
    "**How to fix it**:\n",
    "- Try using **early stopping** (stop training when the validation error increases).\n",
    "- **Regularization**: Add **Ridge** (L2) or **Lasso** (L1) regularization to penalize large weights and reduce overfitting.\n",
    "- Reduce model complexity or the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?\n",
    "\n",
    "No, because **Mini-batch Gradient Descent** introduces more fluctuation in the cost function due to the smaller, random batches of data. The validation error might temporarily increase, but you should wait for a more consistent trend before stopping. **Early stopping** is still a good idea, but don't stop on the first increase in validation error—monitor the trend over several epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)** reaches the vicinity of the optimal solution the fastest because it updates weights more frequently (after each sample).\n",
    "- **Batch Gradient Descent** will actually converge because it uses the whole dataset for each update, leading to more stable convergence.\n",
    "  \n",
    "To make SGD or Mini-batch converge:\n",
    "- Gradually **reduce the learning rate** (learning rate schedules), allowing finer adjustments as the algorithm approaches the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "This indicates **overfitting**: the model is performing well on the training set but not generalizing well to the validation set.\n",
    "\n",
    "Three ways to solve this:\n",
    "1. **Decrease the model complexity**: Use a lower-degree polynomial.\n",
    "2. **Regularization**: Apply **Ridge** or **Lasso** regression to penalize large coefficients.\n",
    "3. **More training data**: Increasing the dataset size can help the model generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter α or reduce it?\n",
    "\n",
    "If both the **training error** and **validation error** are high and almost equal, the model is likely suffering from **high bias** (underfitting).\n",
    "\n",
    "**Solution**: You should **reduce the regularization hyperparameter (α)**. A high α increases bias, so reducing it will allow the model to fit the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Why would you want to use:\n",
    "a. **Ridge Regression** instead of plain Linear Regression?\n",
    "   - Ridge Regression helps avoid **overfitting** by penalizing large coefficients, making the model more robust to noise and preventing it from fitting irrelevant features too closely.\n",
    "\n",
    "b. **Lasso instead of Ridge Regression**?\n",
    "   - **Lasso** can **perform feature selection** by shrinking some coefficients to zero, which is useful when you want a simpler model that ignores irrelevant features.\n",
    "\n",
    "c. **Elastic Net instead of Lasso**?\n",
    "   - **Elastic Net** combines the strengths of both Ridge and Lasso. It is useful when you have many correlated features, as Lasso alone might arbitrarily pick one feature and ignore others. Elastic Net allows for a mix of both penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regression classifier?\n",
    "\n",
    "You should implement **two Logistic Regression classifiers** because you have **two separate binary classification tasks**:\n",
    "1. Outdoor vs. Indoor\n",
    "2. Daytime vs. Nighttime\n",
    "\n",
    "Softmax Regression is more suited for **multiclass classification**, where there are more than two classes in a single task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term to X\n",
    "\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "\n",
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the datasets\n",
    "\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m), y] = 1\n",
    "    return Y_one_hot\n",
    "\n",
    "Y_train_one_hot = to_one_hot(y_train)\n",
    "Y_valid_one_hot = to_one_hot(y_valid)\n",
    "Y_test_one_hot = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax function\n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.446205811872683\n",
      "500 0.8350062641405651\n",
      "1000 0.6878801447192402\n",
      "1500 0.6012379137693313\n",
      "2000 0.5444496861981872\n",
      "2500 0.5038530181431525\n",
      "3000 0.4729228972192248\n",
      "3500 0.44824244188957774\n",
      "4000 0.4278651093928793\n",
      "4500 0.4106007142918712\n",
      "5000 0.3956780375390374\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))\n",
    "\n",
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.629842469083912\n",
      "500 0.5339667976629506\n",
      "1000 0.503640075014894\n",
      "1500 0.4946891059460321\n",
      "2000 0.4912968418075477\n",
      "2500 0.48989924700933296\n",
      "3000 0.4892990598451198\n",
      "3500 0.489035124439786\n",
      "4000 0.4889173621830817\n",
      "4500 0.4888643337449302\n",
      "5000 0.4888403120738818\n"
     ]
    }
   ],
   "source": [
    "# add l2 regularization\n",
    "\n",
    "eta = 0.1\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    Y_proba = softmax(logits)\n",
    "    xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    error = Y_proba - Y_train_one_hot\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "print(accuracy_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
