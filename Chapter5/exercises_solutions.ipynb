{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the fundamental idea behind Support Vector Machines?\n",
    "The fundamental idea behind **Support Vector Machines (SVMs)** is to find the **optimal hyperplane** that best separates data points of different classes in a dataset. For linearly separable data, this hyperplane maximizes the **margin**, which is the distance between the hyperplane and the nearest data points of each class (called support vectors). For non-linearly separable data, SVM uses **kernel functions** to map the data into a higher-dimensional space where it becomes linearly separable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is a support vector?\n",
    "A **support vector** is a data point that lies closest to the decision boundary (or hyperplane). These points are critical in defining the position and orientation of the hyperplane because the margin is maximized based on their distance. Removing a support vector would change the decision boundary, which is why they are called \"support\" vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Why is it important to scale the inputs when using SVMs?\n",
    "It is important to **scale the inputs** (normalize or standardize) when using SVMs because SVMs are sensitive to the scale of the input features. If one feature has a much larger range than others, the SVM will prioritize that feature more, potentially distorting the hyperplane. By scaling the data, you ensure that all features contribute equally to the decision boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?\n",
    "- **Confidence Score**: SVM classifiers can output a **decision function**, which provides a confidence score based on the distance from the hyperplane. However, it is not a probability.\n",
    "- **Probability**: SVMs do not natively provide probabilities, but by using **Platt scaling** (enabled in Scikit-Learn by setting `probability=True`), you can calibrate the SVM’s outputs to provide probability estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?\n",
    "For datasets with **millions of instances and hundreds of features**, you should use the **primal form** of the SVM problem. The **dual form** is typically used when the number of features is larger than the number of training instances, and it becomes computationally expensive for very large datasets. **LinearSVC** in Scikit-Learn solves the primal problem efficiently for large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease γ (gamma)? What about C?\n",
    "- **Increase γ (gamma)**: Increasing gamma makes the model more sensitive to individual data points, meaning the decision boundary becomes more flexible, which could help in reducing underfitting.\n",
    "- **Increase C**: Increasing C reduces the regularization, allowing the model to fit the training data better by penalizing misclassifications less. This too can help reduce underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP solver?\n",
    "To solve the soft-margin linear SVM problem using a Quadratic Programming (QP) solver, you would set the parameters as follows:\n",
    "- **H**: This is the Hessian matrix that corresponds to the quadratic term in the objective function, representing the dot products of the features.\n",
    "- **f**: This is the vector corresponding to the linear term in the objective function, typically set to `-1` for soft-margin SVMs.\n",
    "- **A**: This represents the constraints, typically set to the labels `y` (as `y_i * (w^T x_i + b) >= 1 - ξ_i`).\n",
    "- **b**: This represents the right-hand side of the inequality constraints, typically set to `0` in the soft-margin SVM formulation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and an SGDClassifier on the same dataset. See if you can get them to produce roughly the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC accuracy: 1.0\n",
      "SVC accuracy: 1.0\n",
      "SGDClassifier accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a dataset\n",
    "X, y = make_blobs(n_samples=1000, centers=2, random_state=42)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LinearSVC model\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, y_train)\n",
    "\n",
    "# Train an SVC with linear kernel\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Train an SGDClassifier\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_linear_svc = linear_svc.predict(X_test)\n",
    "y_pred_svc = svc.predict(X_test)\n",
    "y_pred_sgd = sgd.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_linear_svc = accuracy_score(y_test, y_pred_linear_svc)\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "\n",
    "print(f'LinearSVC accuracy: {accuracy_linear_svc}')\n",
    "print(f'SVC accuracy: {accuracy_svc}')\n",
    "print(f'SGDClassifier accuracy: {accuracy_sgd}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Train an SVM classifier on the MNIST dataset.\n",
    "\n",
    "Since SVM classifiers are binary classifiers, you will need to use one-versus-all (OvA) strategy to classify all 10 digits (0 to 9). You can train 10 binary classifiers, one for each digit, and then select the class with the highest decision score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST dataset: 0.963\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the MNISt dataset\n",
    "mnist = datasets.fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist['target']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVC model\n",
    "svc_clf = SVC(kernel='rbf', gamma='scale', C=1)\n",
    "svc_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svc_clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on MNIST dataset: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Train an SVM regressor on the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on California housing dataset: 0.3570026426754465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing['data'], housing['target']\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the dataset\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVR model\n",
    "svr = SVR(kernel='rbf', gamma='scale', C=1)\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svr.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean squared error on California housing dataset: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
